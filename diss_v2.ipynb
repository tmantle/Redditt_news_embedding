{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(client_id='IKnXnWxiUuXhKA',\n",
    "                     client_secret='wvvyNqFcSHiTLOgfTGcI_FIsaYIyew',\n",
    "                     user_agent='tm_quant')\n",
    "\n",
    "headlines = set()\n",
    "\n",
    "for submission in reddit.subreddit('worldnews').top(\"day\",limit=1):\n",
    "    headlines.add(submission.title)\n",
    "    display.clear_output()\n",
    "    print(len(headlines))\n",
    "\n",
    "s = list(headlines)\n",
    "new=s\n",
    "new1=s\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    txt=s[i]\n",
    "    s[i]=s[i].replace('%','percent')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    text = s[i]\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in range(0,len(s)):   \n",
    "    sentence = new[i]\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "    new1[i]=' '.join(edited_sentence)\n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "\n",
    "l = [None] * (len(headlines))\n",
    "\n",
    "for i in range(0,len(headlines)):\n",
    "    nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "    x=new1[i]\n",
    "    output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                     \"outputFormat\": \"json\",\n",
    "                                        \"openie.max_entailments_per_clause\":\"1\"})\n",
    "    \n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    if len(result[0])>0:\n",
    "        z=len(result[0])\n",
    "        relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "        l[i]=list(relationSent)\n",
    "    else:\n",
    "        l[i]=[None,None,None]\n",
    "        \n",
    "df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)\n",
    "\n",
    "\n",
    "df1 = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df1.shape[0]/df.shape[0]\n",
    "\n",
    "triples=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "urls=[]\n",
    "\n",
    "for z in range(0,len(new1)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = new1[z]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        urls = all_urls + urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/SPARQLWrapper/Wrapper.py:979: RuntimeWarning: Sending Accept header '*/*' because unexpected returned format 'json' in a 'DESCRIBE' SPARQL query form\n",
      "  warnings.warn(\"Sending Accept header '*/*' because unexpected returned format '%s' in a '%s' SPARQL query form\" % (self.returnFormat, self.queryType), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,len(headlines)):\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                \n",
    "\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "\n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/birthPlace\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/birthPlace\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/nationality\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/subdivisionName\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/deathPlace\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/location\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/location\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/deathPlace\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/stateOfOrigin\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/residence\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/locationCountry\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/regionServed\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/nationality\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/origin\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/spokenIn\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/hometown\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/states\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/citizenship\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/citizenship\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/populationPlace\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/locatedInArea\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/headquarter\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/residence\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/origin\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/city\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/city\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/president\"]\n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/president\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/hqLocationCountry\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/venue\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/nationalOrigin\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/stadium\"]\n",
    "                \n",
    "               \n",
    "        \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "                frames=[whole_df,extra_df]\n",
    "\n",
    "                whole_df=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test=whole_df.groupby('P').count()\n",
    "#test.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>done</td>\n",
       "      <td>Germany</td>\n",
       "      <td>hit Paris climate targets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>http://dbpedia.org/property/olympicPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Athletics_at_the_1...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>http://dbpedia.org/property/locationSigned</td>\n",
       "      <td>http://dbpedia.org/resource/Treaty_establishin...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>http://dbpedia.org/property/olympicPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Athletics_at_the_1...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>http://dbpedia.org/property/olympicPlace</td>\n",
       "      <td>http://dbpedia.org/resource/Athletics_at_the_1...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94190</td>\n",
       "      <td>http://dbpedia.org/property/seat</td>\n",
       "      <td>http://dbpedia.org/resource/List_of_presidents...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94223</td>\n",
       "      <td>http://dbpedia.org/ontology/training</td>\n",
       "      <td>http://dbpedia.org/resource/Miné_Okubo</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94229</td>\n",
       "      <td>http://dbpedia.org/property/studio</td>\n",
       "      <td>http://dbpedia.org/resource/Tchokola</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94386</td>\n",
       "      <td>http://dbpedia.org/property/locationCity</td>\n",
       "      <td>http://dbpedia.org/resource/PowerJet</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94419</td>\n",
       "      <td>http://dbpedia.org/ontology/administrativeCenter</td>\n",
       "      <td>http://dbpedia.org/resource/Espace_Francophone...</td>\n",
       "      <td>http://dbpedia.org/resource/Paris</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2155 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      P  \\\n",
       "0                                                  done   \n",
       "24             http://dbpedia.org/property/olympicPlace   \n",
       "36           http://dbpedia.org/property/locationSigned   \n",
       "80             http://dbpedia.org/property/olympicPlace   \n",
       "86             http://dbpedia.org/property/olympicPlace   \n",
       "...                                                 ...   \n",
       "94190                  http://dbpedia.org/property/seat   \n",
       "94223              http://dbpedia.org/ontology/training   \n",
       "94229                http://dbpedia.org/property/studio   \n",
       "94386          http://dbpedia.org/property/locationCity   \n",
       "94419  http://dbpedia.org/ontology/administrativeCenter   \n",
       "\n",
       "                                                       S  \\\n",
       "0                                                Germany   \n",
       "24     http://dbpedia.org/resource/Athletics_at_the_1...   \n",
       "36     http://dbpedia.org/resource/Treaty_establishin...   \n",
       "80     http://dbpedia.org/resource/Athletics_at_the_1...   \n",
       "86     http://dbpedia.org/resource/Athletics_at_the_1...   \n",
       "...                                                  ...   \n",
       "94190  http://dbpedia.org/resource/List_of_presidents...   \n",
       "94223             http://dbpedia.org/resource/Miné_Okubo   \n",
       "94229               http://dbpedia.org/resource/Tchokola   \n",
       "94386               http://dbpedia.org/resource/PowerJet   \n",
       "94419  http://dbpedia.org/resource/Espace_Francophone...   \n",
       "\n",
       "                                       O  \n",
       "0              hit Paris climate targets  \n",
       "24     http://dbpedia.org/resource/Paris  \n",
       "36     http://dbpedia.org/resource/Paris  \n",
       "80     http://dbpedia.org/resource/Paris  \n",
       "86     http://dbpedia.org/resource/Paris  \n",
       "...                                  ...  \n",
       "94190  http://dbpedia.org/resource/Paris  \n",
       "94223  http://dbpedia.org/resource/Paris  \n",
       "94229  http://dbpedia.org/resource/Paris  \n",
       "94386  http://dbpedia.org/resource/Paris  \n",
       "94419  http://dbpedia.org/resource/Paris  \n",
       "\n",
       "[2155 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df=pd.concat([df1, whole_df], sort=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "graph_df=final_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = graph_df['S'].tolist()\n",
    "target = graph_df['O'].tolist()\n",
    "edge = graph_df['P'].tolist()\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':edge})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.nx.draw_networkx(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "nx.draw_networkx_edge_labels(G, pos=pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ampligraph.latent_features import TransE\n",
    "triple=whole_df.values\n",
    "model = TransE(batches_count=1, seed=555, epochs=20, k=10, loss='pairwise',loss_params={'margin':5})\n",
    "X = triple\n",
    "model.fit(X)\n",
    "\n",
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
