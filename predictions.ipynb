{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "start_epoch=int(dt.datetime(2021,4,1).timestamp())\n",
    "end_epoch1 = int(dt.datetime(2021, 7,30).timestamp())\n",
    "\n",
    "list1=list(api.search_submissions(after=start_epoch,before =end_epoch1, \n",
    "                            subreddit='worldnews',\n",
    "                            filter=['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=[None]*len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(list1)):\n",
    "    new1[i]=list1[i][3]['title']\n",
    "\n",
    "s=new1\n",
    "new=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(s)):\n",
    "    txt=s[i]\n",
    "    s[i]=s[i].replace('%','percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    text = s[i]\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in range(0,len(s)):   \n",
    "    sentence = new[i]\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "    new1[i]=' '.join(edited_sentence)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=list(filter(lambda a: a != '', new1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "\n",
    "l = [None] * (len(new1))\n",
    "\n",
    "for i in range(0,(len(new1))):\n",
    "    nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "    x=new1[i]\n",
    "    output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                     \"outputFormat\": \"json\",\n",
    "                                        \"openie.max_entailments_per_clause\":\"1\"})\n",
    "    \n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    if len(result[0])>0:\n",
    "        z=len(result[0])\n",
    "        relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "        l[i]=list(relationSent)\n",
    "    else:\n",
    "        l[i]=[None,None,None]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.reverse()  \n",
    "df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4798793859649123"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df1.shape[0]/df.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train=df1[:math.floor(0.75*len(df1))]\n",
    "valid=df1[math.floor(0.75*len(df1)):math.floor(0.90*len(df1))]\n",
    "test=df1[math.floor(0.9*len(df1)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples=train.values\n",
    "valid_triples=valid.values\n",
    "test_triples=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "urls=[]\n",
    "\n",
    "for z in range(0,len(train_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = train_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        urls = all_urls + urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "test_urls=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SeekableUnicodeStreamReader.__del__ at 0x7fe27aad7320>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\", line 1280, in __del__\n",
      "    if not self.closed:\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/nltk/data.py\", line 1294, in closed\n",
      "    return self.stream.closed\n",
      "AttributeError: 'SeekableUnicodeStreamReader' object has no attribute 'stream'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(test_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + test_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                frames=[whole_df,extra_df]\n",
    "\n",
    "                whole_df=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/ontology/Automobile/wheelbase</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/firstFlightDate</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/firstHatTrickPlayer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/firstHolder</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/firstLaunch</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/location</td>\n",
       "      <td>172780</td>\n",
       "      <td>172780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://www.w3.org/2002/07/owl#sameAs</td>\n",
       "      <td>262779</td>\n",
       "      <td>262779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/birthPlace</td>\n",
       "      <td>281440</td>\n",
       "      <td>281440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>418852</td>\n",
       "      <td>418852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/ontology/country</td>\n",
       "      <td>455076</td>\n",
       "      <td>455076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8337 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       S       O\n",
       "P                                                               \n",
       "http://dbpedia.org/ontology/Automobile/wheelbase       1       1\n",
       "http://dbpedia.org/property/firstFlightDate            1       1\n",
       "http://dbpedia.org/property/firstHatTrickPlayer        1       1\n",
       "http://dbpedia.org/property/firstHolder                1       1\n",
       "http://dbpedia.org/property/firstLaunch                1       1\n",
       "...                                                  ...     ...\n",
       "http://dbpedia.org/property/location              172780  172780\n",
       "http://www.w3.org/2002/07/owl#sameAs              262779  262779\n",
       "http://dbpedia.org/property/birthPlace            281440  281440\n",
       "http://dbpedia.org/ontology/birthPlace            418852  418852\n",
       "http://dbpedia.org/ontology/country               455076  455076\n",
       "\n",
       "[8337 rows x 2 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=whole_df.groupby('P').count()\n",
    "test.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "valid_urls=[]\n",
    "\n",
    "for z in range(0,len(valid_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = valid_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        valid_urls = all_urls + valid_urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    valid_urls = list(dict.fromkeys(valid_urls))\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    valid_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(valid_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + valid_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                \n",
    "                frames=[valid_df,extra_df]\n",
    "\n",
    "                valid_df=pd.concat(frames)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([train, whole_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final_df=pd.concat([valid, valid_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"S\"] = final_df[\"S\"].astype(str)\n",
    "final_df[\"P\"] = final_df[\"P\"].astype(str)\n",
    "final_df[\"O\"] = final_df[\"O\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final_df[\"S\"] = valid_final_df[\"S\"].astype(str)\n",
    "valid_final_df[\"P\"] = valid_final_df[\"P\"].astype(str)\n",
    "valid_final_df[\"O\"] = valid_final_df[\"O\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test2=valid_final_df.groupby('P').count()\n",
    "#test2=test2.sort_values(by=['O'])\n",
    "#first_v=test2.iloc[:, 0]\n",
    "#relations_valid=first_v.index.values.tolist() , \n",
    "#list(set(relations_valid) - set(relations_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['S','P','O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final_df = valid_final_df[['S','P','O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rel=final_df.groupby('P').count()\n",
    "remove_rel=remove_rel.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>P</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Analogue</td>\n",
       "      <td>’s</td>\n",
       "      <td>Pocket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>E!</td>\n",
       "      <td>has</td>\n",
       "      <td>Erin Lim Rhodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Amy Winehouse</td>\n",
       "      <td>has</td>\n",
       "      <td>Mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Biden</td>\n",
       "      <td>’s</td>\n",
       "      <td>Big Bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>PingPong</td>\n",
       "      <td>is</td>\n",
       "      <td>video chat app for product teams working acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>http://dbpedia.org/resource/Alexa_Internet</td>\n",
       "      <td>http://dbpedia.org/property/currentStatus</td>\n",
       "      <td>Online</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>http://dbpedia.org/resource/Alexa_Internet</td>\n",
       "      <td>http://www.w3.org/2002/07/owl#sameAs</td>\n",
       "      <td>http://lt.dbpedia.org/resource/Alexa_Internet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>http://dbpedia.org/resource/Alexa_Internet</td>\n",
       "      <td>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</td>\n",
       "      <td>http://dbpedia.org/class/yago/Abstraction10000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>http://dbpedia.org/resource/Alexa_Internet</td>\n",
       "      <td>http://www.w3.org/2002/07/owl#sameAs</td>\n",
       "      <td>http://ja.dbpedia.org/resource/アレクサ・インターネット</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>http://dbpedia.org/resource/Alexa_Internet</td>\n",
       "      <td>http://www.w3.org/2002/07/owl#sameAs</td>\n",
       "      <td>http://hu.dbpedia.org/resource/Alexa_Internet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4507610 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              S  \\\n",
       "15                                     Analogue   \n",
       "16                                           E!   \n",
       "17                                Amy Winehouse   \n",
       "18                                        Biden   \n",
       "19                                     PingPong   \n",
       "..                                          ...   \n",
       "854  http://dbpedia.org/resource/Alexa_Internet   \n",
       "861  http://dbpedia.org/resource/Alexa_Internet   \n",
       "866  http://dbpedia.org/resource/Alexa_Internet   \n",
       "868  http://dbpedia.org/resource/Alexa_Internet   \n",
       "872  http://dbpedia.org/resource/Alexa_Internet   \n",
       "\n",
       "                                                   P  \\\n",
       "15                                                ’s   \n",
       "16                                               has   \n",
       "17                                               has   \n",
       "18                                                ’s   \n",
       "19                                                is   \n",
       "..                                               ...   \n",
       "854        http://dbpedia.org/property/currentStatus   \n",
       "861             http://www.w3.org/2002/07/owl#sameAs   \n",
       "866  http://www.w3.org/1999/02/22-rdf-syntax-ns#type   \n",
       "868             http://www.w3.org/2002/07/owl#sameAs   \n",
       "872             http://www.w3.org/2002/07/owl#sameAs   \n",
       "\n",
       "                                                     O  \n",
       "15                                              Pocket  \n",
       "16                                     Erin Lim Rhodes  \n",
       "17                                                 Mom  \n",
       "18                                             Big Bet  \n",
       "19   video chat app for product teams working acros...  \n",
       "..                                                 ...  \n",
       "854                                             Online  \n",
       "861      http://lt.dbpedia.org/resource/Alexa_Internet  \n",
       "866  http://dbpedia.org/class/yago/Abstraction10000...  \n",
       "868        http://ja.dbpedia.org/resource/アレクサ・インターネット  \n",
       "872      http://hu.dbpedia.org/resource/Alexa_Internet  \n",
       "\n",
       "[4507610 rows x 3 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_rel=remove_rel.loc[remove_rel['S'] < 10]\n",
    "single_rel=single_rel.index.values.tolist()\n",
    "\n",
    "final_df=final_df[~final_df['P'].isin(single_rel)]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove relations in validation set which do not exist in training set\n",
    "\n",
    "test1=final_df.groupby('P').count()\n",
    "test1=test1.sort_values(by=['O'])\n",
    "first_c=test1.iloc[:, 0]\n",
    "relations_train=first_c.index.values.tolist()\n",
    "\n",
    "#valid_final_df=valid_final_df[valid_final_df['P'].isin(relations_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=final_df.groupby('S').count()\n",
    "test2=test2.sort_values(by=['O'])\n",
    "first_c_s=test2.iloc[:, 0]\n",
    "subject_train=first_c_s.index.values.tolist()\n",
    "#valid_final_df=valid_final_df[valid_final_df['S'].isin(subject_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3=final_df.groupby('O').count()\n",
    "test3=test3.sort_values(by=['S'])\n",
    "first_c_o=test3.iloc[:, 0]\n",
    "object_train=first_c_o.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "ampligraph.latent_features.set_entity_threshold(2276568)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.035257: 100%|██████████| 30/30 [3:01:21<00:00, 362.73s/epoch]  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "train_all_triple=final_df.values\n",
    "#valid_all_triple=valid_final_df.values\n",
    "\n",
    "model = TransE(batches_count=64, seed=0, epochs=30, k=100, eta=40,optimizer='adam', optimizer_params={'lr':0.0001},loss='pairwise', verbose=True)\n",
    "\n",
    "model.fit(train_all_triple) #, early_stopping=True, early_stopping_params={'x_valid':valid_all_triple,'burn_in':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = 'helloworld.pkl'\n",
    "ampligraph.utils.save_model(model, model_name_path = example_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle(\"final_df.pkl\")\n",
    "valid_final_df.to_pickle(\"valid_final_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Daily Headline Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.date(2021, 7, 1)\n",
    "end_date = dt.date(2021, 8, 8)\n",
    "x=(end_date - start_date)\n",
    "avg_scores=[None]*(x.days) \n",
    "dayy=0\n",
    "\n",
    "while start_date < end_date:\n",
    "    \n",
    "    next_day=start_date+delta\n",
    "    reddit = praw.Reddit(\n",
    "    client_id='IKnXnWxiUuXhKA',\n",
    "                         client_secret='wvvyNqFcSHiTLOgfTGcI_FIsaYIyew',\n",
    "                         user_agent='tm_quant'\n",
    "    )\n",
    "    api = PushshiftAPI(reddit)\n",
    "\n",
    "    start_epoch = int(pd.Timestamp(start_date).timestamp())\n",
    "    end_epoch = int(pd.Timestamp(next_day).timestamp())\n",
    "\n",
    "    gen = api.search_submissions(\n",
    "      subreddit = 'worldnews',\n",
    "      filter = ['id'],\n",
    "      after = start_epoch,\n",
    "      before = end_epoch\n",
    "    )\n",
    "\n",
    "    results = list(gen)\n",
    "\n",
    "\n",
    "    titles=results\n",
    "    for i in range(0,len(titles)):\n",
    "        titles[i]=titles[i].title\n",
    "\n",
    "    reddit = praw.Reddit(\n",
    "      client_id='IKnXnWxiUuXhKA',\n",
    "                         client_secret='wvvyNqFcSHiTLOgfTGcI_FIsaYIyew',\n",
    "                         user_agent='tm_quant'\n",
    "    )\n",
    "    api = PushshiftAPI(reddit)\n",
    "\n",
    "    start_epoch = int(pd.Timestamp(start_date).timestamp())\n",
    "    end_epoch = int(pd.Timestamp(next_day).timestamp())\n",
    "\n",
    "    gen = api.search_submissions(\n",
    "      subreddit = 'worldnews',\n",
    "      filter = ['id'],\n",
    "      after = start_epoch,\n",
    "      before = end_epoch\n",
    "    )\n",
    "\n",
    "    results = list(gen)\n",
    "\n",
    "    scores=results\n",
    "    for r in range(0,len(titles)):\n",
    "        scores[r]=scores[r].score\n",
    "\n",
    "\n",
    "    #x=len(scores)\n",
    "    #y=len(titles)\n",
    "    #scores=(scores[(x-len(titles)):x])\n",
    "\n",
    "    todays_news_dict = {'Title':titles,'Score':scores}\n",
    "    todays_news = pd.DataFrame(todays_news_dict)\n",
    "\n",
    "    todays_news=todays_news.sort_values(by=['Score'], ascending=False)\n",
    "\n",
    "    todays_news=todays_news.Title.values.tolist()\n",
    "\n",
    "    top_news=todays_news[0:50]\n",
    "\n",
    "    headlines=top_news\n",
    "    s = top_news\n",
    "    new=s\n",
    "    new1=s\n",
    "\n",
    "    for y in range(0,len(s)):\n",
    "        txt=s[y]\n",
    "        s[y]=s[y].replace('%','percent')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "    for u in range(0,len(s)):\n",
    "        text = s[u]\n",
    "        text_tokens = word_tokenize(text)\n",
    "\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        new[u]=' '.join(tokens_without_sw)\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "    for c in range(0,len(s)):   \n",
    "        sentence = new[c]\n",
    "        tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "        edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "        new1[c]=' '.join(edited_sentence)\n",
    "\n",
    "    import nltk\n",
    "    from pycorenlp import *\n",
    "    import collections\n",
    "\n",
    "    l = [None] * (len(headlines))\n",
    "\n",
    "    for g in range(0,len(headlines)):\n",
    "        nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "        x=new1[g]\n",
    "        output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                         \"outputFormat\": \"json\",\n",
    "                                            \"openie.max_entailments_per_clause\":\"1\"})\n",
    "\n",
    "        result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "        if len(result[0])>0:\n",
    "            z=len(result[0])\n",
    "            relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "            l[g]=list(relationSent)\n",
    "        else:\n",
    "            l[g]=[None,None,None]\n",
    "\n",
    "    day_df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)\n",
    "    day_df = day_df.replace(to_replace='None', value=np.nan).dropna()\n",
    "\n",
    "\n",
    "\n",
    "    day_triples=day_df.values\n",
    "\n",
    "    import json\n",
    "    from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "    import requests\n",
    "    import urllib.parse\n",
    "\n",
    "    day_urls=[]\n",
    "\n",
    "    for p in range(0,len(day_triples)):\n",
    "        try:\n",
    "            ## initial consts\n",
    "            BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "            TEXT = day_triples[p][2]\n",
    "            CONFIDENCE = '0.6'\n",
    "            SUPPORT = '10'\n",
    "            REQUEST = BASE_URL.format(\n",
    "                text=urllib.parse.quote_plus(TEXT), \n",
    "                confidence=CONFIDENCE, \n",
    "                support=SUPPORT\n",
    "            )\n",
    "            HEADERS = {'Accept': 'application/json'}\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            all_urls = []\n",
    "\n",
    "            r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "            response = r.json()\n",
    "            resources = response['Resources']\n",
    "            for res in resources:\n",
    "                all_urls.append(res['@URI'])\n",
    "\n",
    "            day_urls = all_urls + day_urls\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass # doing nothing on exception\n",
    "        \n",
    "\n",
    "    day_urls = list(dict.fromkeys(day_urls))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_day_df= pd.DataFrame(data,columns=columns)\n",
    "    for m in range(0,len(day_urls)):\n",
    "\n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + day_urls[m] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "\n",
    "\n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for t in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[t]=results['results']['bindings'][t]['s']['value']\n",
    "                    p_l[t]=results['results']['bindings'][t]['p']['value']\n",
    "                    o_l[t]=results['results']['bindings'][t]['o']['value']\n",
    "\n",
    "\n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                                   columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[t:]\n",
    "\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "\n",
    "\n",
    "                \n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "\n",
    "\n",
    "\n",
    "                frames=[whole_day_df,extra_df]\n",
    "\n",
    "                whole_day_df=pd.concat(frames)\n",
    "\n",
    "    whole_day_df=whole_day_df[whole_day_df['P'].isin(relations_train)]\n",
    "    whole_day_df=whole_day_df[whole_day_df['S'].isin(subject_train)]\n",
    "    whole_day_df=whole_day_df[whole_day_df['O'].isin(object_train)]\n",
    "\n",
    "    ##check=whole_day_df.groupby('O').count()\n",
    "    #check=check.sort_values(by=['S'])\n",
    "    #check_s=check.iloc[:, 0]\n",
    "    #relations_day=check_s.index.values.tolist()\n",
    "    #list(set(relations_day) - set(object_train))\n",
    "\n",
    "    day_triples_final=whole_day_df.values\n",
    "\n",
    "    \n",
    "    if len(day_triples_final)==0:\n",
    "        avg_scores[dayy]=0\n",
    "    \n",
    "    else:\n",
    "        scores = model.predict(day_triples_final)\n",
    "        import statistics\n",
    "        avg_scores[dayy]=statistics.mean(scores)\n",
    "    \n",
    "        \n",
    "    start_date += delta\n",
    "    dayy=dayy+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.date(2021, 7, 1)\n",
    "end_date = dt.date(2021, 8, 8)\n",
    "\n",
    "\n",
    "number_of_days = end_date-start_date\n",
    "number_of_days=number_of_days.days\n",
    "\n",
    "\n",
    "date_list = []\n",
    "for day in range(number_of_days):\n",
    "    a_date = (start_date + dt.timedelta(days = day)).isoformat()\n",
    "    date_list.append(a_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = pd.DataFrame(\n",
    "    {'Date': date_list,\n",
    "     'score': avg_scores\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df['Date']= pd.to_datetime(daily_df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "msft = yf.Ticker(\"AAPL\")\n",
    "\n",
    "# get historical market data\n",
    "hist = msft.history(period=\"27d\")\n",
    "\n",
    "date_price = hist[[\"Close\"]]\n",
    "date_price['Date'] = date_price.index\n",
    "date_price=date_price[['Date','Close']]\n",
    "date_price.index = range(0, len(date_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_lst=hist.Close.tolist()\n",
    "\n",
    "up_down=price_lst[:-1]\n",
    "for p in range(0,len(price_lst)-1):\n",
    "    if price_lst[p+1]>price_lst[p]:\n",
    "        up_down[p]=1\n",
    "    else:\n",
    "        up_down[p]=-1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_daily_df=pd.merge(date_price, daily_df, on='Date')\n",
    "\n",
    "final_daily_df['up_down'] = up_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Prediction</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_daily_df.drop([\"Date\",'Close','up_down'], axis=1)\n",
    "y = final_daily_df.up_down\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=1121218\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:01:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "model = SVC() #[0.1, 1, 10, 100, 1000]\n",
    "# defining parameter range \n",
    "param_grid = {'C': [0.1, 0.01, 0.001],  \n",
    "              'gamma': [0.1,0.01,0.001],\n",
    "              'degree':[2,3],\n",
    "              'coef0':np.linspace(-200,200,20),\n",
    "              'kernel': ['rbf', 'linear','poly']}  \n",
    "  \n",
    "grid = GridSearchCV(SVC(), param_grid, scoring='recall_macro') \n",
    "  \n",
    "# fitting the model for grid search \n",
    "grid.fit(X_train, y_train) \n",
    "\n",
    "grid_predictions = grid.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
