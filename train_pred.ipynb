{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.date(2020, 7, 1)\n",
    "start=start_date\n",
    "end_date = dt.date(2021, 8, 13)\n",
    "\n",
    "number_of_days = end_date-start_date\n",
    "number_of_days=number_of_days.days\n",
    "loop=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "start_date = dt.date(2021, 5, 15)\n",
    "start=start_date\n",
    "end_date = dt.date(2021, 8, 13)\n",
    "\n",
    "\n",
    "number_of_days = end_date-start_date\n",
    "number_of_days=number_of_days.days\n",
    "loop=0\n",
    "\n",
    "date_list = []\n",
    "for day in range(number_of_days):\n",
    "    a_date = (start_date + dt.timedelta(days = day)).isoformat()\n",
    "    date_list.append(a_date)\n",
    "    \n",
    "dates_df = pd.DataFrame(\n",
    "    {'Date': date_list\n",
    "    })\n",
    "\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "import datetime as dt\n",
    "\n",
    "api = PushshiftAPI()\n",
    "delta=dt.timedelta(days=1)\n",
    "triples_list=[None]*number_of_days\n",
    "\n",
    "while start_date < (end_date):\n",
    "\n",
    "    start_epoch=int(pd.Timestamp(start_date).timestamp())\n",
    "    next_day=start_date+delta\n",
    "    end_epoch1 =int(pd.Timestamp(next_day).timestamp())\n",
    "    list1=list(api.search_submissions(after=start_epoch,\n",
    "                                      before =end_epoch1, \n",
    "                                subreddit='worldnews',\n",
    "                                filter=['title','score']))\n",
    "\n",
    "\n",
    "\n",
    "    titles=[None]*len(list1)\n",
    "    scores_all=[None]*len(list1)\n",
    "    for i in range(0,len(list1)):\n",
    "        titles[i]=list1[i][4]['title']\n",
    "        scores_all[i]=list1[i][4]['score']\n",
    "\n",
    "    daily_news = {'Title':titles,'Score':scores_all}\n",
    "    daily_news_df = pd.DataFrame(daily_news)\n",
    "    daily_news_df=daily_news_df.sort_values(by=['Score'], ascending=False)\n",
    "    daily_news_df=daily_news_df.Title.values.tolist()\n",
    "    daily_news_df=daily_news_df[0:25]\n",
    "    new1=daily_news_df\n",
    "    s=new1\n",
    "    new=s\n",
    "\n",
    "\n",
    "    for i in range(0,len(s)):\n",
    "        txt=s[i]\n",
    "        s[i]=s[i].replace('%','percent')\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    for i in range(0,len(s)):\n",
    "        text = s[i]\n",
    "        text_tokens = word_tokenize(text)\n",
    "\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "    import nltk\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "    for i in range(0,len(s)):   \n",
    "        sentence = new[i]\n",
    "        tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "        edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "        new1[i]=' '.join(edited_sentence)\n",
    "\n",
    "    new1=list(filter(lambda a: a != '', new1))\n",
    "\n",
    "    l = [None] * (len(new1))\n",
    "\n",
    "    for i in range(0,(len(new1))):\n",
    "        nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "        x=new1[i]\n",
    "        output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n",
    "                                             \"triple.strict\":\"true\",\"outputFormat\": \"json\",\n",
    "                                             \"openie.max_entailments_per_clause\":\"1\"})\n",
    "\n",
    "        result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "        if len(result[0])>0:\n",
    "            z=len(result[0])\n",
    "            relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "            l[i]=list(relationSent)\n",
    "        else:\n",
    "            l[i]=[None,None,None]\n",
    "\n",
    "    df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)\n",
    "    df1 = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "    triples=df1.values\n",
    "    all_data=df1.values\n",
    "\n",
    "\n",
    "    urls=[]\n",
    "\n",
    "\n",
    "    for z in range(0,len(all_data)):\n",
    "        try:\n",
    "            ## initial consts\n",
    "            BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "            TEXT = all_data[z][2]\n",
    "            CONFIDENCE = '0.7'\n",
    "            SUPPORT = '10'\n",
    "            REQUEST = BASE_URL.format(\n",
    "                text=urllib.parse.quote_plus(TEXT), \n",
    "                confidence=CONFIDENCE, \n",
    "                support=SUPPORT\n",
    "            )\n",
    "            HEADERS = {'Accept': 'application/json'}\n",
    "            sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "            all_urls = []\n",
    "\n",
    "            r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "            response = r.json()\n",
    "            resources = response['Resources']\n",
    "            for res in resources:\n",
    "                all_urls.append(res['@URI'])\n",
    "\n",
    "            urls = all_urls + urls\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass # doing nothing on exception\n",
    "\n",
    "\n",
    "    urls = list(dict.fromkeys(urls))\n",
    "    test_urls=urls\n",
    "\n",
    "\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(test_urls)):\n",
    "\n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + test_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "\n",
    "\n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "\n",
    "\n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                                   columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "\n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                frames=[whole_df,extra_df]\n",
    "\n",
    "                whole_df=pd.concat(frames)\n",
    "\n",
    "    date_triples = pd.DataFrame(\n",
    "        {'Date': [start_date.isoformat()], 'triples':[whole_df.values]\n",
    "        })\n",
    "    \n",
    "    \n",
    "    triples_list[loop]=whole_df.values\n",
    "    start_date=start_date+delta\n",
    "    loop=loop+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_triples_dates = pd.DataFrame(\n",
    "        {'Date': date_list, 'triples_':triples_list\n",
    "         \n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_with_triples=pd.concat([dff1,dff2,all_triples_dates])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day_with_triples=pd.concat([df11,df22,all_triples_dates])\n",
    "#dff1\n",
    "#dff2\n",
    "#day_with_triples=day_with_triples.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_with_triples=day_with_triples.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day_with_triples['Date'] = pd.to_datetime(day_with_triples['Date'])\n",
    "day_with_triples=all_triples_dates\n",
    "day_with_triples = day_with_triples.set_index(day_with_triples['Date'])\n",
    "day_with_triples = day_with_triples.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>triples_</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>[[http://dbpedia.org/resource/Ab_Khvor, http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[[http://dbpedia.org/resource/Space_Patrol_(19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>[[http://dbpedia.org/resource/Ditaola_Ditaola,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>[[http://dbpedia.org/resource/Celia_Viveros, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>[[http://dbpedia.org/resource/Productivity_Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>[[http://dbpedia.org/resource/1982_bombing_of_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>[[http://dbpedia.org/resource/Patience_Torlowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>2021-08-10</td>\n",
       "      <td>[[http://dbpedia.org/resource/Emily_Ratajkowsk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-08-11</td>\n",
       "      <td>2021-08-11</td>\n",
       "      <td>[[http://dbpedia.org/resource/Retina, http://d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>[[http://dbpedia.org/resource/Ab_Khvor, http:/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                           triples_\n",
       "Date                                                                     \n",
       "2020-07-01  2020-07-01  [[http://dbpedia.org/resource/Ab_Khvor, http:/...\n",
       "2020-07-02  2020-07-02  [[http://dbpedia.org/resource/Space_Patrol_(19...\n",
       "2020-07-03  2020-07-03  [[http://dbpedia.org/resource/Ditaola_Ditaola,...\n",
       "2020-07-04  2020-07-04  [[http://dbpedia.org/resource/Celia_Viveros, h...\n",
       "2020-07-05  2020-07-05  [[http://dbpedia.org/resource/Productivity_Com...\n",
       "...                ...                                                ...\n",
       "2021-08-08  2021-08-08  [[http://dbpedia.org/resource/1982_bombing_of_...\n",
       "2021-08-09  2021-08-09  [[http://dbpedia.org/resource/Patience_Torlowe...\n",
       "2021-08-10  2021-08-10  [[http://dbpedia.org/resource/Emily_Ratajkowsk...\n",
       "2021-08-11  2021-08-11  [[http://dbpedia.org/resource/Retina, http://d...\n",
       "2021-08-12  2021-08-12  [[http://dbpedia.org/resource/Ab_Khvor, http:/...\n",
       "\n",
       "[408 rows x 2 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_with_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Train model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: (266, 2)\n",
      "Valid Dataset: (12, 2)\n",
      "Test Dataset: (130, 2)\n"
     ]
    }
   ],
   "source": [
    "train = day_with_triples[start.strftime('%Y-%m-%d'):(start+(dt.timedelta(days=number_of_days*0.65))).strftime('%Y-%m-%d')]\n",
    "valid=day_with_triples[(start+(dt.timedelta(days=number_of_days*0.65))+(dt.timedelta(days=1))).strftime('%Y-%m-%d'):(start+(dt.timedelta(days=number_of_days*0.68))).strftime('%Y-%m-%d')]\n",
    "test  = day_with_triples[(start+(dt.timedelta(days=number_of_days*0.68))+(dt.timedelta(days=1))).strftime('%Y-%m-%d'):]\n",
    "print('Train Dataset:',train.shape)\n",
    "print('Valid Dataset:',valid.shape)\n",
    "print('Test Dataset:',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=train['triples_'][0]\n",
    "for zz in range(1,len(train['triples_'])):\n",
    "    if len(train['triples_'][zz])==0:\n",
    "        continue\n",
    "    else:\n",
    "        arr = np.concatenate((arr,train['triples_'][zz]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_arr=valid['triples_'][0]\n",
    "for vv in range(1,len(valid['triples_'])):\n",
    "    if len(valid['triples_'][vv])==0:\n",
    "        continue\n",
    "    else:\n",
    "        val_arr = np.concatenate((val_arr,valid['triples_'][vv]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_of_train = pd.DataFrame(arr, \n",
    "             columns=['S', \n",
    "                      'P','O'])\n",
    "\n",
    "df_of_valid = pd.DataFrame(val_arr, \n",
    "             columns=['S', \n",
    "                      'P','O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=df_of_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final_df=df_of_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"S\"] = final_df[\"S\"].astype(str)\n",
    "final_df[\"P\"] = final_df[\"P\"].astype(str)\n",
    "final_df[\"O\"] = final_df[\"O\"].astype(str)\n",
    "\n",
    "valid_final_df[\"S\"] = valid_final_df[\"S\"].astype(str)\n",
    "valid_final_df[\"P\"] = valid_final_df[\"P\"].astype(str)\n",
    "valid_final_df[\"O\"] = valid_final_df[\"O\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rel=final_df.groupby('P').count()\n",
    "remove_rel=remove_rel.sort_values(by=['O'])\n",
    "\n",
    "single_rel=remove_rel.loc[remove_rel['S'] < 10]\n",
    "single_rel=single_rel.index.values.tolist()\n",
    "\n",
    "final_df=final_df[~final_df['P'].isin(single_rel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove relations in validation set which do not exist in training set\n",
    "\n",
    "test1=final_df.groupby('P').count()\n",
    "test1=test1.sort_values(by=['O'])\n",
    "first_c=test1.iloc[:, 0]\n",
    "relations_train=first_c.index.values.tolist()\n",
    "\n",
    "#valid_final_df=valid_final_df[valid_final_df['P'].isin(relations_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=final_df.groupby('S').count()\n",
    "test2=test2.sort_values(by=['O'])\n",
    "first_c_s=test2.iloc[:, 0]\n",
    "subject_train=first_c_s.index.values.tolist()\n",
    "#valid_final_df=valid_final_df[valid_final_df['S'].isin(subject_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3=final_df.groupby('O').count()\n",
    "test3=test3.sort_values(by=['S'])\n",
    "first_c_o=test3.iloc[:, 0]\n",
    "object_train=first_c_o.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_final_df=valid_final_df[valid_final_df['S'].isin(subject_train)]\n",
    "valid_final_df=valid_final_df[valid_final_df['P'].isin(relations_train)]\n",
    "valid_final_df=valid_final_df[valid_final_df['O'].isin(object_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv('train_data.csv')\n",
    "#valid_final_df.to_csv('valid_data.csv')\n",
    "#test.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#final_df=pd.read_csv('train_data.csv')\n",
    "\n",
    "#final_df=final_df[['S','P','O']]\n",
    "#final_df[\"S\"] = final_df[\"S\"].astype(str)\n",
    "#final_df[\"P\"] = final_df[\"P\"].astype(str)\n",
    "#final_df[\"O\"] = final_df[\"O\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "ampligraph.latent_features.set_entity_threshold(1658808)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.024827: 100%|██████████| 30/30 [4:17:16<00:00, 514.54s/epoch]  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ampligraph\n",
    "\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "train_all_triple=final_df.values\n",
    "#valid_all_triple=valid_final_df.values\n",
    "\n",
    "model = TransE(batches_count=64, seed=0, epochs=30, k=100, eta=20,optimizer='adam', optimizer_params={'lr':0.0001},loss='pairwise', verbose=True)\n",
    "\n",
    "model.fit(train_all_triple) #, early_stopping=True, early_stopping_params={'x_valid':valid_all_triple,'burn_in':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = 'helloworld.pkl'\n",
    "ampligraph.utils.save_model(model, model_name_path = example_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Embed Daily</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train['s_overall_av']=None\n",
    "train['p_overall_av']=None\n",
    "train['o_overall_av']=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>triples_</th>\n",
       "      <th>s_overall_av</th>\n",
       "      <th>p_overall_av</th>\n",
       "      <th>o_overall_av</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>2020-07-01</td>\n",
       "      <td>[[http://dbpedia.org/resource/Ab_Khvor, http:/...</td>\n",
       "      <td>0.000257778</td>\n",
       "      <td>-0.00064785</td>\n",
       "      <td>0.000298753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>2020-07-02</td>\n",
       "      <td>[[http://dbpedia.org/resource/Space_Patrol_(19...</td>\n",
       "      <td>-0.000863363</td>\n",
       "      <td>0.00316943</td>\n",
       "      <td>0.00169336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>2020-07-03</td>\n",
       "      <td>[[http://dbpedia.org/resource/Ditaola_Ditaola,...</td>\n",
       "      <td>-0.00016974</td>\n",
       "      <td>-0.000199955</td>\n",
       "      <td>-0.000406729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>[[http://dbpedia.org/resource/Celia_Viveros, h...</td>\n",
       "      <td>-3.62698e-05</td>\n",
       "      <td>-0.000577759</td>\n",
       "      <td>0.000238436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>2020-07-05</td>\n",
       "      <td>[[http://dbpedia.org/resource/Productivity_Com...</td>\n",
       "      <td>-0.000352343</td>\n",
       "      <td>-0.00062766</td>\n",
       "      <td>-7.58505e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>2021-03-19</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-03-20</td>\n",
       "      <td>2021-03-20</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>2021-03-21</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2021-03-23</td>\n",
       "      <td>2021-03-23</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date                                           triples_  \\\n",
       "Date                                                                        \n",
       "2020-07-01  2020-07-01  [[http://dbpedia.org/resource/Ab_Khvor, http:/...   \n",
       "2020-07-02  2020-07-02  [[http://dbpedia.org/resource/Space_Patrol_(19...   \n",
       "2020-07-03  2020-07-03  [[http://dbpedia.org/resource/Ditaola_Ditaola,...   \n",
       "2020-07-04  2020-07-04  [[http://dbpedia.org/resource/Celia_Viveros, h...   \n",
       "2020-07-05  2020-07-05  [[http://dbpedia.org/resource/Productivity_Com...   \n",
       "...                ...                                                ...   \n",
       "2021-03-19  2021-03-19                                                 []   \n",
       "2021-03-20  2021-03-20                                                 []   \n",
       "2021-03-21  2021-03-21                                                 []   \n",
       "2021-03-22  2021-03-22                                                 []   \n",
       "2021-03-23  2021-03-23                                                 []   \n",
       "\n",
       "           s_overall_av p_overall_av o_overall_av  \n",
       "Date                                               \n",
       "2020-07-01  0.000257778  -0.00064785  0.000298753  \n",
       "2020-07-02 -0.000863363   0.00316943   0.00169336  \n",
       "2020-07-03  -0.00016974 -0.000199955 -0.000406729  \n",
       "2020-07-04 -3.62698e-05 -0.000577759  0.000238436  \n",
       "2020-07-05 -0.000352343  -0.00062766 -7.58505e-05  \n",
       "...                 ...          ...          ...  \n",
       "2021-03-19            0            0            0  \n",
       "2021-03-20            0            0            0  \n",
       "2021-03-21            0            0            0  \n",
       "2021-03-22            0            0            0  \n",
       "2021-03-23            0            0            0  \n",
       "\n",
       "[266 rows x 5 columns]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dayy in range(0,len(train)):\n",
    "    daily_triple_df = pd.DataFrame(train['triples_'][dayy], \n",
    "             columns=['S', \n",
    "                      'P','O'])\n",
    "    remove_rel=df_of_train.groupby('P').count()\n",
    "    remove_rel=remove_rel.sort_values(by=['O'])\n",
    "\n",
    "    single_rel=remove_rel.loc[remove_rel['S'] < 10]\n",
    "    single_rel=single_rel.index.values.tolist()\n",
    "\n",
    "    daily_triple_df=daily_triple_df[~daily_triple_df['P'].isin(single_rel)]\n",
    "\n",
    "    daily_triple_df['s_avg_emb']=None\n",
    "    daily_triple_df['p_avg_emb']=None\n",
    "    daily_triple_df['o_avg_emb']=None\n",
    "    daily_triple_df=daily_triple_df.reset_index(drop=True)\n",
    "    \n",
    "    if len(daily_triple_df.S.values.tolist())==0:\n",
    "        train['s_overall_av'][dayy]=0\n",
    "        train['p_overall_av'][dayy]=0\n",
    "        train['o_overall_av'][dayy]=0\n",
    "    else:    \n",
    "        \n",
    "        train['s_overall_av'][dayy]=np.average(model.get_embeddings(daily_triple_df.S.values.tolist(),embedding_type='entity'))\n",
    "        train['p_overall_av'][dayy]=np.average(model.get_embeddings(daily_triple_df.P.values.tolist(),embedding_type='relation'))\n",
    "        train['o_overall_av'][dayy]=np.average(model.get_embeddings(daily_triple_df.O.values.tolist(),embedding_type='entity'))\n",
    "    print(train['s_overall_av'][dayy])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['s_overall_av']=None\n",
    "test['p_overall_av']=None\n",
    "test['o_overall_av']=None\n",
    "for dayy2 in range(0,len(test)):   \n",
    "    daily_triple_df = pd.DataFrame(test['triples_'][dayy2], \n",
    "             columns=['S', \n",
    "                      'P','O'])\n",
    "\n",
    "\n",
    "    daily_triple_df=daily_triple_df[daily_triple_df['S'].isin(subject_train)]\n",
    "    daily_triple_df=daily_triple_df[daily_triple_df['P'].isin(relations_train)]\n",
    "    daily_triple_df=daily_triple_df[daily_triple_df['O'].isin(object_train)]\n",
    "    daily_triple_df['s_avg_emb']=None\n",
    "    daily_triple_df['p_avg_emb']=None\n",
    "    daily_triple_df['o_avg_emb']=None\n",
    "    \n",
    "    if len(daily_triple_df.S.values.tolist())==0:\n",
    "        test['s_overall_av'][dayy2]=0\n",
    "        test['p_overall_av'][dayy2]=0\n",
    "        test['o_overall_av'][dayy2]=0\n",
    "    else:    \n",
    "        test['s_overall_av'][dayy2]=np.average(model.get_embeddings(daily_triple_df.S.values.tolist(),embedding_type='entity'))\n",
    "        test['p_overall_av'][dayy2]=np.average(model.get_embeddings(daily_triple_df.P.values.tolist(),embedding_type='relation'))\n",
    "        test['o_overall_av'][dayy2]=np.average(model.get_embeddings(daily_triple_df.O.values.tolist(),embedding_type='entity'))\n",
    "    print(test['o_overall_av'][dayy2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Add finance data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "# get historical market data\n",
    "hist = yf.download(\"AAPL\", start=start.strftime('%Y-%m-%d'), end=end_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "date_price = hist[[\"Close\"]]\n",
    "date_price['Date'] = date_price.index\n",
    "date_price=date_price[['Date','Close']]\n",
    "date_price.index = range(0, len(date_price))\n",
    "#date_price=date_price.set_index('Date', inplace=True)\n",
    "date_price.index=date_price['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_price_train=pd.merge(date_price, train, left_index=True, right_index=True)\n",
    "date_price_train=date_price_train[['triples_','s_overall_av', 'p_overall_av','o_overall_av','Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_price_test=pd.merge(date_price, test, left_index=True, right_index=True)\n",
    "date_price_test=date_price_y=date_price_test[['triples_','s_overall_av', 'p_overall_av','o_overall_av','Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_price_train['up_down']=None\n",
    "for p in range(0,len(date_price_train)-1):\n",
    "    if date_price_train['Close'][p+1]>date_price_train['Close'][p]:\n",
    "        date_price_train['up_down'][p]=1\n",
    "    else:\n",
    "        date_price_train['up_down'][p]=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_price_test['up_down']=None\n",
    "for p in range(0,len(date_price_test)-1):\n",
    "    if date_price_test['Close'][p+1]>date_price_test['Close'][p]:\n",
    "        date_price_test['up_down'][p]=1\n",
    "    else:\n",
    "        date_price_test['up_down'][p]=-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Prediction</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_cl = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_price_train=date_price_train.fillna(0)\n",
    "date_price_test=date_price_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = date_price_train.drop([\"triples_\",'Close','up_down'], axis=1)[:-1]\n",
    "y_train = date_price_train.up_down[:-1]\n",
    "\n",
    "X_test= date_price_test.drop([\"triples_\",'Close','up_down'], axis=1)[:-1]\n",
    "y_test= date_price_test.up_down[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('xtrain.npy', X_train)\n",
    "np.save('ytrain.npy', y_train)\n",
    "np.save('xtest.npy', X_test)\n",
    "np.save('ytest.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:04:57] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5604395604395604"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "xgb_cl = xgb.XGBClassifier()\n",
    "\n",
    "# Fit\n",
    "xgb_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "preds = xgb_cl.predict(X_test)\n",
    "\n",
    "# Score\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
