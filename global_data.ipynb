{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/psaw/PushshiftAPI.py:192: UserWarning: Got non 200 code 429\n",
      "  warnings.warn(\"Got non 200 code %s\" % response.status_code)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/psaw/PushshiftAPI.py:180: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "start_epoch=int(dt.datetime(2021,6,20).timestamp())\n",
    "\n",
    "list1=list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit='worldnews',\n",
    "                            filter=['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=[None]*len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(list1)):\n",
    "    new1[i]=list1[i][3]['title']\n",
    "\n",
    "s=new1\n",
    "new=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(s)):\n",
    "    txt=s[i]\n",
    "    s[i]=s[i].replace('%','percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    text = s[i]\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in range(0,len(s)):   \n",
    "    sentence = new[i]\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "    new1[i]=' '.join(edited_sentence)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=list(filter(lambda a: a != '', new1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "\n",
    "l = [None] * (len(new1))\n",
    "\n",
    "for i in range(0,(len(new1))):\n",
    "    nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "    x=new1[i]\n",
    "    output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                     \"outputFormat\": \"json\",\n",
    "                                        \"openie.max_entailments_per_clause\":\"1\"})\n",
    "    \n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    if len(result[0])>0:\n",
    "        z=len(result[0])\n",
    "        relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "        l[i]=list(relationSent)\n",
    "    else:\n",
    "        l[i]=[None,None,None]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.reverse()  \n",
    "df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4819107929515419"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df1.shape[0]/df.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train=df1[:math.floor(0.75*len(df1))]\n",
    "valid=df1[math.floor(0.75*len(df1)):math.floor(0.90*len(df1))]\n",
    "test=df1[math.floor(0.9*len(df1)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples=train.values\n",
    "valid_triples=valid.values\n",
    "test_triples=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['thousands', 'Ministers eye technology', 'NHS staffing hours'],\n",
       "       ['cut', 'Rising inflation', 'household incomes'],\n",
       "       ['hit', 'UK aviation', 'travel rules'],\n",
       "       ...,\n",
       "       ['decimates', 'wildfire U.S.', 'Northern California town'],\n",
       "       ['injuring', 'Man', '10 Tokyo train'],\n",
       "       ['mark', 'googul Kirk Gibson # Hosting # Golf outing fight',\n",
       "        'googul.xyz']], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "urls=[]\n",
    "\n",
    "for z in range(0,len(train_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = train_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        urls = all_urls + urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "test_urls=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/SPARQLWrapper/Wrapper.py:979: RuntimeWarning: Sending Accept header '*/*' because unexpected returned format 'json' in a 'DESCRIBE' SPARQL query form\n",
      "  warnings.warn(\"Sending Accept header '*/*' because unexpected returned format '%s' in a '%s' SPARQL query form\" % (self.returnFormat, self.queryType), RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(test_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + test_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                \n",
    "                \n",
    "                \n",
    "                frames=[whole_df,extra_df]\n",
    "\n",
    "                whole_df=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/ontology/Automobile/wheelbase</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/logo2Alt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/logo2Caption</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/logoImage</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/logoPic</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/subdivisionName</td>\n",
       "      <td>127333</td>\n",
       "      <td>127333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/location</td>\n",
       "      <td>148734</td>\n",
       "      <td>148734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://www.w3.org/2002/07/owl#sameAs</td>\n",
       "      <td>154101</td>\n",
       "      <td>154101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/property/birthPlace</td>\n",
       "      <td>241047</td>\n",
       "      <td>241047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>354757</td>\n",
       "      <td>354757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6752 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       S       O\n",
       "P                                                               \n",
       "http://dbpedia.org/ontology/Automobile/wheelbase       1       1\n",
       "http://dbpedia.org/property/logo2Alt                   1       1\n",
       "http://dbpedia.org/property/logo2Caption               1       1\n",
       "http://dbpedia.org/property/logoImage                  1       1\n",
       "http://dbpedia.org/property/logoPic                    1       1\n",
       "...                                                  ...     ...\n",
       "http://dbpedia.org/property/subdivisionName       127333  127333\n",
       "http://dbpedia.org/property/location              148734  148734\n",
       "http://www.w3.org/2002/07/owl#sameAs              154101  154101\n",
       "http://dbpedia.org/property/birthPlace            241047  241047\n",
       "http://dbpedia.org/ontology/birthPlace            354757  354757\n",
       "\n",
       "[6752 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=whole_df.groupby('P').count()\n",
    "test.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "valid_urls=[]\n",
    "\n",
    "for z in range(0,len(valid_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = valid_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        valid_urls = all_urls + valid_urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "    valid_urls = list(dict.fromkeys(valid_urls))\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    valid_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(valid_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + valid_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                \n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                \n",
    "                frames=[valid_df,extra_df]\n",
    "\n",
    "                valid_df=pd.concat(frames)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([train, whole_df], sort=False)\n",
    "valid_final_df=pd.concat([valid, valid_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"S\"] = final_df[\"S\"].astype(str)\n",
    "final_df[\"P\"] = final_df[\"P\"].astype(str)\n",
    "final_df[\"O\"] = final_df[\"O\"].astype(str)\n",
    "\n",
    "valid_final_df[\"S\"] = valid_final_df[\"S\"].astype(str)\n",
    "valid_final_df[\"P\"] = valid_final_df[\"P\"].astype(str)\n",
    "valid_final_df[\"O\"] = valid_final_df[\"O\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test2=valid_final_df.groupby('P').count()\n",
    "#test2=test2.sort_values(by=['O'])\n",
    "#first_v=test2.iloc[:, 0]\n",
    "#relations_valid=first_v.index.values.tolist() , \n",
    "#list(set(relations_valid) - set(relations_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['S','P','O']]\n",
    "valid_final_df = valid_final_df[['S','P','O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rel=final_df.groupby('P').count()\n",
    "remove_rel=remove_rel.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_rel=remove_rel.loc[remove_rel['S'] < 100]\n",
    "single_rel=single_rel.index.values.tolist()\n",
    "\n",
    "final_df=final_df[~final_df['P'].isin(single_rel)]\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove relations in validation set which do not exist in training set\n",
    "\n",
    "test1=final_df.groupby('P').count()\n",
    "test1=test1.sort_values(by=['O'])\n",
    "first_c=test1.iloc[:, 0]\n",
    "relations_train=first_c.index.values.tolist()\n",
    "valid_final_df=valid_final_df[valid_final_df['P'].isin(relations_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2=final_df.groupby('S').count()\n",
    "test2=test2.sort_values(by=['O'])\n",
    "first_c_s=test2.iloc[:, 0]\n",
    "subject_train=first_c_s.index.values.tolist()\n",
    "valid_final_df=valid_final_df[valid_final_df['S'].isin(subject_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3=final_df.groupby('O').count()\n",
    "test3=test3.sort_values(by=['S'])\n",
    "first_c_o=test3.iloc[:, 0]\n",
    "object_train=first_c_o.index.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ampligraph\n",
    "ampligraph.latent_features.set_entity_threshold(1542585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.019796: 100%|██████████| 40/40 [2:44:57<00:00, 247.45s/epoch]  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "train_all_triple=final_df.values\n",
    "valid_all_triple=valid_final_df.values\n",
    "\n",
    "model = TransE(batches_count=64, seed=0, epochs=40, k=100, eta=40,optimizer='adam', optimizer_params={'lr':0.0001},loss='pairwise', verbose=True)\n",
    "\n",
    "model.fit(train_all_triple) #, early_stopping=True, early_stopping_params={'x_valid':valid_all_triple,'burn_in':100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = 'helloworld.pkl'\n",
    "ampligraph.utils.save_model(model, model_name_path = example_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle(\"final_df.pkl\")\n",
    "valid_final_df.to_pickle(\"valid_final_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"7\">Daily Headline Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "  client_id='IKnXnWxiUuXhKA',\n",
    "                     client_secret='wvvyNqFcSHiTLOgfTGcI_FIsaYIyew',\n",
    "                     user_agent='tm_quant'\n",
    ")\n",
    "api = PushshiftAPI(reddit)\n",
    "\n",
    "start_epoch = int(dt.datetime(2021, 8, 8).timestamp())\n",
    "end_epoch = int(dt.datetime(2021, 8, 9).timestamp())\n",
    "\n",
    "gen = api.search_submissions(\n",
    "  subreddit = 'worldnews',\n",
    "  filter = ['id'],\n",
    "  after = start_epoch,\n",
    "  before = end_epoch\n",
    ")\n",
    "\n",
    "results = list(gen)\n",
    "\n",
    "\n",
    "titles=results\n",
    "for i in range(0,len(titles)):\n",
    "    titles[i]=titles[i].title\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "  client_id='IKnXnWxiUuXhKA',\n",
    "                     client_secret='wvvyNqFcSHiTLOgfTGcI_FIsaYIyew',\n",
    "                     user_agent='tm_quant'\n",
    ")\n",
    "api = PushshiftAPI(reddit)\n",
    "\n",
    "start_epoch = int(dt.datetime(2021, 8, 8).timestamp())\n",
    "end_epoch = int(dt.datetime(2021, 8, 9).timestamp())\n",
    "\n",
    "gen = api.search_submissions(\n",
    "  subreddit = 'worldnews',\n",
    "  filter = ['id'],\n",
    "  after = start_epoch,\n",
    "  before = end_epoch\n",
    ")\n",
    "\n",
    "results = list(gen)\n",
    "\n",
    "scores=results\n",
    "for i in range(0,len(titles)):\n",
    "    scores[i]=scores[i].score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=len(scores)\n",
    "#y=len(titles)\n",
    "#scores=(scores[(x-len(titles)):x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_news_dict = {'Title':titles,'Score':scores}\n",
    "todays_news = pd.DataFrame(todays_news_dict)\n",
    "\n",
    "todays_news=todays_news.sort_values(by=['Score'], ascending=False)\n",
    "\n",
    "todays_news=todays_news.Title.values.tolist()\n",
    "\n",
    "top_news=todays_news[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "headlines=top_news\n",
    "s = top_news\n",
    "new=s\n",
    "new1=s\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    txt=s[i]\n",
    "    s[i]=s[i].replace('%','percent')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    text = s[i]\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in range(0,len(s)):   \n",
    "    sentence = new[i]\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "    new1[i]=' '.join(edited_sentence)\n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "\n",
    "l = [None] * (len(headlines))\n",
    "\n",
    "for i in range(0,len(headlines)):\n",
    "    nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "    x=new1[i]\n",
    "    output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                     \"outputFormat\": \"json\",\n",
    "                                        \"openie.max_entailments_per_clause\":\"1\"})\n",
    "    \n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    if len(result[0])>0:\n",
    "        z=len(result[0])\n",
    "        relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "        l[i]=list(relationSent)\n",
    "    else:\n",
    "        l[i]=[None,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)\n",
    "day_df = day_df.replace(to_replace='None', value=np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_triples=day_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "day_urls=[]\n",
    "\n",
    "for z in range(0,len(day_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = day_triples[z][2]\n",
    "        CONFIDENCE = '0.7'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        day_urls = all_urls + day_urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_urls = list(dict.fromkeys(day_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_day_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(day_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + day_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                \n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                \n",
    "                \n",
    "                \n",
    "                frames=[whole_day_df,extra_df]\n",
    "\n",
    "                whole_day_df=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_day_df=whole_day_df[whole_day_df['P'].isin(relations_train)]\n",
    "whole_day_df=whole_day_df[whole_day_df['S'].isin(subject_train)]\n",
    "whole_day_df=whole_day_df[whole_day_df['O'].isin(object_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check=whole_day_df.groupby('O').count()\n",
    "#check=check.sort_values(by=['S'])\n",
    "#check_s=check.iloc[:, 0]\n",
    "#relations_day=check_s.index.values.tolist()\n",
    "#list(set(relations_day) - set(object_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_triples=whole_day_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(day_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.6190782"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
