{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "start_epoch=int(dt.datetime(2021,7,20).timestamp())\n",
    "\n",
    "list1=list(api.search_submissions(after=start_epoch,\n",
    "                            subreddit='worldnews',\n",
    "                            filter=['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1=[None]*len(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(list1)):\n",
    "    new1[i]=list1[i][3]['title']\n",
    "\n",
    "s=new1\n",
    "new=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(s)):\n",
    "    txt=s[i]\n",
    "    s[i]=s[i].replace('%','percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tristanmantle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "for i in range(0,len(s)):\n",
    "    text = s[i]\n",
    "    text_tokens = word_tokenize(text)\n",
    "\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    new[i]=' '.join(tokens_without_sw)\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "for i in range(0,len(s)):   \n",
    "    sentence = new[i]\n",
    "    tagged_sentence = nltk.tag.pos_tag(sentence.split())\n",
    "    edited_sentence = [word for word,tag in tagged_sentence if tag != 'JJ' and tag != 'JJR' and tag != 'JJS' and tag != 'RB' and tag != 'RBR' and tag != 'RBS']\n",
    "    new1[i]=' '.join(edited_sentence)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new1=list(filter(lambda a: a != '', new1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "import nltk\n",
    "from pycorenlp import *\n",
    "import collections\n",
    "\n",
    "l = [None] * (len(new1))\n",
    "\n",
    "for i in range(0,(len(new1))):\n",
    "    nlp=StanfordCoreNLP('http://localhost:9000/')\n",
    "    x=new1[i]\n",
    "    output = nlp.annotate(x, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\"triple.strict\":\"true\",\n",
    "                                     \"outputFormat\": \"json\",\n",
    "                                        \"openie.max_entailments_per_clause\":\"1\"})\n",
    "    \n",
    "    result = [output[\"sentences\"][0][\"openie\"] for item in output]\n",
    "    if len(result[0])>0:\n",
    "        z=len(result[0])\n",
    "        relationSent=result[0][z-1]['relation'],result[0][z-1]['subject'],result[0][z-1]['object']\n",
    "        l[i]=list(relationSent)\n",
    "    else:\n",
    "        l[i]=[None,None,None]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.reverse()  \n",
    "df = pd.DataFrame(l, columns =['P', 'S', 'O'], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4663002603413364"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.replace(to_replace='None', value=np.nan).dropna()\n",
    "df1.shape[0]/df.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['stop', 'Jerry', 'sales territories'],\n",
       "       ['jumps', 'gymnast', 'Olympic Village bed news'],\n",
       "       ['tossed', \"Peru 's authority\",\n",
       "        'leaving Pedro Castillo doorstep presidency'],\n",
       "       ...,\n",
       "       ['killed', 'family seven', 'Turkey'],\n",
       "       ['killed', 'Iran attack tanker Oman', 'two'],\n",
       "       ['killed', 'family seven', 'Turkey']], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "train=df1[:math.floor(0.75*len(df1))]\n",
    "valid=df1[math.floor(0.75*len(df1)):math.floor(0.90*len(df1))]\n",
    "test=df1[math.floor(0.9*len(df1)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triples=train.values\n",
    "valid_triples=valid.values\n",
    "test_triples=test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>S</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>stop</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>sales territories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>jumps</td>\n",
       "      <td>gymnast</td>\n",
       "      <td>Olympic Village bed news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>tossed</td>\n",
       "      <td>Peru 's authority</td>\n",
       "      <td>leaving Pedro Castillo doorstep presidency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Reviews For</td>\n",
       "      <td>Reviews</td>\n",
       "      <td>Best Product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>visit India at_time</td>\n",
       "      <td>Afghan Army Chief</td>\n",
       "      <td>week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6908</td>\n",
       "      <td>offer</td>\n",
       "      <td>sanctions Syria faction</td>\n",
       "      <td>solace victims</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6910</td>\n",
       "      <td>meets</td>\n",
       "      <td>Cuba sanctions Biden</td>\n",
       "      <td>Cuban leaders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6911</td>\n",
       "      <td>killed</td>\n",
       "      <td>family seven</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6912</td>\n",
       "      <td>killed</td>\n",
       "      <td>Iran attack tanker Oman</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6913</td>\n",
       "      <td>killed</td>\n",
       "      <td>family seven</td>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3224 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        P                        S  \\\n",
       "1                    stop                    Jerry   \n",
       "2                   jumps                  gymnast   \n",
       "5                  tossed        Peru 's authority   \n",
       "7             Reviews For                  Reviews   \n",
       "8     visit India at_time        Afghan Army Chief   \n",
       "...                   ...                      ...   \n",
       "6908                offer  sanctions Syria faction   \n",
       "6910                meets     Cuba sanctions Biden   \n",
       "6911               killed             family seven   \n",
       "6912               killed  Iran attack tanker Oman   \n",
       "6913               killed             family seven   \n",
       "\n",
       "                                               O  \n",
       "1                              sales territories  \n",
       "2                       Olympic Village bed news  \n",
       "5     leaving Pedro Castillo doorstep presidency  \n",
       "7                                   Best Product  \n",
       "8                                           week  \n",
       "...                                          ...  \n",
       "6908                              solace victims  \n",
       "6910                               Cuban leaders  \n",
       "6911                                      Turkey  \n",
       "6912                                         two  \n",
       "6913                                      Turkey  \n",
       "\n",
       "[3224 rows x 3 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data=df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "urls=[]\n",
    "\n",
    "for z in range(0,len(train_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = train_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        urls = all_urls + urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = list(dict.fromkeys(urls))\n",
    "test_urls=urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    whole_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(test_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + test_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                \n",
    "                \n",
    "                \n",
    "                frames=[whole_df,extra_df]\n",
    "\n",
    "                whole_df=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test=whole_df.groupby('P').count()\n",
    "#test.sort_values(by=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import requests\n",
    "import urllib.parse\n",
    "\n",
    "valid_urls=[]\n",
    "\n",
    "for z in range(0,len(valid_triples)):\n",
    "    try:\n",
    "        ## initial consts\n",
    "        BASE_URL = 'http://api.dbpedia-spotlight.org/en/annotate?text={text}&confidence={confidence}&support={support}'\n",
    "        TEXT = valid_triples[z][2]\n",
    "        CONFIDENCE = '0.9'\n",
    "        SUPPORT = '10'\n",
    "        REQUEST = BASE_URL.format(\n",
    "            text=urllib.parse.quote_plus(TEXT), \n",
    "            confidence=CONFIDENCE, \n",
    "            support=SUPPORT\n",
    "        )\n",
    "        HEADERS = {'Accept': 'application/json'}\n",
    "        sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "        all_urls = []\n",
    "\n",
    "        r = requests.get(url=REQUEST, headers=HEADERS)\n",
    "        response = r.json()\n",
    "        resources = response['Resources']\n",
    "        for res in resources:\n",
    "            all_urls.append(res['@URI'])\n",
    "        \n",
    "        valid_urls = all_urls + valid_urls\n",
    "\n",
    "    except:\n",
    "        \n",
    "        pass # doing nothing on exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "    valid_urls = list(dict.fromkeys(valid_urls))\n",
    "    columns = ['S','P', 'O']\n",
    "    data = []\n",
    "    valid_df= pd.DataFrame(data,columns=columns)\n",
    "    for i in range(0,len(valid_urls)):\n",
    "                \n",
    "                line = 'DESCRIBE '\n",
    "                output_line=line +'<' + valid_urls[i] + '>'\n",
    "\n",
    "\n",
    "\n",
    "                sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "\n",
    "                q=\"\"\"\n",
    "\n",
    "                    {}\n",
    "\n",
    "                \"\"\".format(output_line)\n",
    "\n",
    "                sparql.setQuery(q)\n",
    "\n",
    "\n",
    "\n",
    "                sparql.setReturnFormat(JSON)\n",
    "                results = sparql.query().convert()\n",
    "                \n",
    "                \n",
    "\n",
    "                s_l = [None] * (len(results['results']['bindings']))\n",
    "                p_l = [None] * (len(results['results']['bindings']))\n",
    "                o_l = [None] * (len(results['results']['bindings']))\n",
    "                for i in range(1,(len(results['results']['bindings']))):\n",
    "\n",
    "                    s_l[i]=results['results']['bindings'][i]['s']['value']\n",
    "                    p_l[i]=results['results']['bindings'][i]['p']['value']\n",
    "                    o_l[i]=results['results']['bindings'][i]['o']['value']\n",
    "                    \n",
    "                    \n",
    "\n",
    "                extra_df = pd.DataFrame(list(zip(s_l, p_l,o_l)),\n",
    "                               columns =['S', 'P','O'])\n",
    "                extra_df=extra_df.iloc[1:]\n",
    "                \n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageWikiLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageExternalLink']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageLength']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/ontology/wikiPageRedirects']\n",
    "                extra_df = extra_df[extra_df.P != 'http://dbpedia.org/property/wikiPageUsesTemplate']\n",
    "                \n",
    "                \n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/ontology/country\"]\n",
    "                extra_df=extra_df.loc[extra_df['P'] != \"http://dbpedia.org/property/country\"]\n",
    "                \n",
    "                extra_df['O'] = extra_df['O'].astype(str)\n",
    "                extra_df=extra_df[extra_df['S'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['P'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                extra_df=extra_df[extra_df['O'].str.contains(\"http://dbpedia.org/\",na=False)]\n",
    "                \n",
    "                frames=[valid_df,extra_df]\n",
    "\n",
    "                valid_df=pd.concat(frames)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.concat([train, whole_df], sort=False)\n",
    "valid_final_df=pd.concat([valid, valid_df], sort=False)\n",
    "\n",
    "train_all_triple=final_df.values\n",
    "valid_all_triple=valid_final_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average TransE Loss:   0.033788: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [11:41<00:00, 70.19s/epoch]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "model = TransE(batches_count=64, seed=0, epochs=10, k=100, eta=20,optimizer='adam', optimizer_params={'lr':0.0001},loss='pairwise', verbose=True)\n",
    "\n",
    "model.fit(train_all_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-273-917b9f981dab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pairwise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_all_triple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_all_triple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ampligraph.latent_features import TransE\n",
    "\n",
    "model = TransE(batches_count=64, seed=0, epochs=3, k=100, eta=20,optimizer='adam', optimizer_params={'lr':0.0001},loss='pairwise', verbose=True)\n",
    "\n",
    "model.fit(train_all_triple, early_stopping=True, early_stopping_params={(valid_all_triple[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
